{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:18.820331Z",
     "start_time": "2024-01-12T17:24:18.750265Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "11221"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:19.719148Z",
     "start_time": "2024-01-12T17:24:19.689666Z"
    }
   },
   "id": "b7e86a465e0dc2c6"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "def split(X, y, seed, z=None, train_size=0.7, test_size=0.3, val_size=0.2):\n",
    "    X_val = None\n",
    "    y_val = None\n",
    "    Z_train, Z_val, Z_test = None, None, None\n",
    "    if train_size is None and test_size is None:\n",
    "        raise AttributeError()\n",
    "    elif train_size is None:\n",
    "        train_size = 1. - test_size\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=seed, shuffle=True,\n",
    "                                                        train_size=train_size)\n",
    "    if z is not None:\n",
    "        Z_train, Z_test = train_test_split(z, stratify=y, random_state=seed, shuffle=True, train_size=train_size)\n",
    "    if val_size is not None:\n",
    "        if z is not None:\n",
    "            Z_train, Z_val = train_test_split(Z_train, stratify=y_train, random_state=seed, shuffle=True,\n",
    "                                              test_size=val_size)\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, random_state=seed,\n",
    "                                                          shuffle=True, test_size=val_size)\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), (Z_train, Z_val, Z_test)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:19.722114Z",
     "start_time": "2024-01-12T17:24:19.718701Z"
    }
   },
   "id": "eb50b307ded8b664"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "        df: str | pd.DataFrame = 'dataset/EDOS_1M_balanced.pkl',\n",
    "        seed=0\n",
    "):\n",
    "    df = df\n",
    "    if isinstance(df, str):\n",
    "        if df.endswith('pkl'):\n",
    "            df = pd.read_pickle(df)\n",
    "        else:\n",
    "            df = pd.read_csv(df)\n",
    "    groups = []\n",
    "    labels = []\n",
    "    confidence = []\n",
    "    for name, group in df.groupby(by='eb+_emot'):\n",
    "        group = group.reset_index(drop=True)\n",
    "        groups.append(group.loc[:, 'uttr'])\n",
    "        confidence.append(group.loc[:, 'label_confidence'])\n",
    "        labels.append(group.loc[:, 'eb+_emot'])\n",
    "\n",
    "    groups = np.array(groups, dtype=str)\n",
    "    labels = np.array(labels, dtype=str)\n",
    "\n",
    "    confidence = np.array(confidence, dtype=np.float32)\n",
    "    return groups, labels, confidence\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:19.724340Z",
     "start_time": "2024-01-12T17:24:19.722337Z"
    }
   },
   "id": "e7dc8591360ed07d"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:19.885415Z",
     "start_time": "2024-01-12T17:24:19.725177Z"
    }
   },
   "id": "3220d0df74d38f4c"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "groups, labels, confidence = get_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:20.216409Z",
     "start_time": "2024-01-12T17:24:19.886139Z"
    }
   },
   "id": "2ad6eaacbec9714e"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "\n",
    "unique = np.unique(labels.flatten())\n",
    "emotions_to_category = {key: value for key, value in zip(unique, range(len(unique)))}\n",
    "category_to_emotion = {key: value for key, value in zip(range(len(unique)), unique)}\n",
    "\n",
    "\n",
    "def label_convert(label):\n",
    "    return emotions_to_category[label]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:20.265732Z",
     "start_time": "2024-01-12T17:24:20.219252Z"
    }
   },
   "id": "14e6ad56c5716510"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "labels = np.vectorize(label_convert)(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:20.356970Z",
     "start_time": "2024-01-12T17:24:20.270984Z"
    }
   },
   "id": "1262937f9fb1f3c9"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "class OurDataset(Dataset):\n",
    "    def __init__(self, X, y, z):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.X[item], self.y[item], self.z[item]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:20.360260Z",
     "start_time": "2024-01-12T17:24:20.304750Z"
    }
   },
   "id": "9b84c3bbd527b73c"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196950,)\n",
      "(105510,)\n",
      "(49238,)\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), (Z_train, Z_val, Z_test) = split(groups.flatten(),\n",
    "                                                                                       labels.flatten(), seed,\n",
    "                                                                                       confidence.flatten())\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:21.237935Z",
     "start_time": "2024-01-12T17:24:20.482959Z"
    }
   },
   "id": "4e8052b972245e00"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "train_dataset = OurDataset(X_train, y_train, Z_train)\n",
    "test_dataset = OurDataset(X_test, y_test, Z_test)\n",
    "val_dataset = OurDataset(X_val, y_val, Z_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:21.243903Z",
     "start_time": "2024-01-12T17:24:21.238616Z"
    }
   },
   "id": "c0b30139629325f0"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    global tokenizer\n",
    "    x, y, z = [], [], []\n",
    "    for x1, y1, z1 in batch:\n",
    "        x.append(x1)\n",
    "        y.append(y1)\n",
    "        z.append(z1)\n",
    "    batch = tokenizer(x, padding=True, return_tensors='pt')\n",
    "    y = torch.as_tensor(y)\n",
    "    z = torch.as_tensor(z, dtype=torch.float32)\n",
    "    return batch, y, z\n",
    "\n",
    "    # print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:24:21.244671Z",
     "start_time": "2024-01-12T17:24:21.241613Z"
    }
   },
   "id": "34493289a5cb036d"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(0)\n",
    "sampling_train = torch.utils.data.RandomSampler(train_dataset, num_samples=10000, replacement=False, generator=generator)\n",
    "sampling_test = torch.utils.data.RandomSampler(test_dataset, num_samples=1000, replacement=False, generator=generator)\n",
    "sampling_val = torch.utils.data.RandomSampler(val_dataset, num_samples=1000, replacement=False, generator=generator)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn, sampler=sampling_train)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, sampler=sampling_test)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, sampler=sampling_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:15.001075Z",
     "start_time": "2024-01-12T17:36:14.997023Z"
    }
   },
   "id": "b8cf4740bbc38b72"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "class Agglomerate:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class Concat(Agglomerate):\n",
    "    def __init__(self,device, name='bert-base-uncased'):\n",
    "        super(Concat, self).__init__(device)\n",
    "        config = AutoConfig.from_pretrained(name)\n",
    "        config.update({\"output_hidden_states\": True})\n",
    "        self.encoder = AutoModel.from_pretrained(name, config=config)\n",
    "        self.encoder.to(device)\n",
    "        self.encoder.eval()\n",
    "        for param in self.encoder.parameters(recurse=True):\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(batch['input_ids'].to(self.device),\n",
    "                             batch['attention_mask'].to(self.device))  # TODO: check against ** batch\n",
    "            hidden_states = torch.stack(x[\"hidden_states\"])\n",
    "            concatenated = torch.cat((hidden_states[-1], hidden_states[-2], hidden_states[-3], hidden_states[-4]), -1)\n",
    "            concatenated = concatenated[:, 0]\n",
    "        return concatenated\n",
    "    \n",
    "class WeightedPooling(Agglomerate):\n",
    "    def __init__(self, device, start_layer=4, name='bert-base-uncased'):\n",
    "        super(WeightedPooling, self).__init__(device)\n",
    "        config = AutoConfig.from_pretrained(name)\n",
    "        config.update({\"output_hidden_states\": True})\n",
    "        self.encoder = AutoModel.from_pretrained(name, config=config)\n",
    "        self.encoder.to(device)\n",
    "        self.encoder.eval()\n",
    "        for param in self.encoder.parameters(recurse=True):\n",
    "            param.requires_grad = False\n",
    "        self.start_layer = start_layer\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(batch['input_ids'].to(self.device),\n",
    "                             batch['attention_mask'].to(self.device))  # TODO: check against ** batch\n",
    "            hidden_states = torch.stack(x[\"hidden_states\"])\n",
    "            hidden_states = hidden_states[self.start_layer:, :, :, :]\n",
    "        return hidden_states\n",
    "    \n",
    "\n",
    "class EmotionClassifierConcat(nn.Module):\n",
    "    def __init__(self, output_size, device, hidden_layers, activation_fn=nn.ReLU, dropout_value=0.):\n",
    "        super(EmotionClassifierConcat, self).__init__()\n",
    "        self.classification = nn.Sequential(\n",
    "        )\n",
    "        for i in range(len(hidden_layers)):\n",
    "            curr = hidden_layers[i]\n",
    "            if i == 0:\n",
    "                self.classification.append(nn.LazyLinear(curr))\n",
    "                self.classification.append(activation_fn())\n",
    "                if dropout_value > 0:\n",
    "                    self.classification.append(nn.Dropout(dropout_value))\n",
    "                continue\n",
    "            prev = hidden_layers[i - 1]\n",
    "            self.classification.append(nn.Linear(prev, curr))\n",
    "            self.classification.append(activation_fn())\n",
    "            if dropout_value > 0:\n",
    "                self.classification.append(nn.Dropout(dropout_value))\n",
    "\n",
    "        self.classification.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        # self.classification.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, concatenated):\n",
    "        return self.classification(concatenated)\n",
    "\n",
    "\n",
    "class EmotionClassifierWeightedPooling(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 device,\n",
    "                 hidden_layers,\n",
    "                 start_layer,\n",
    "                 activation_fn=nn.ReLU,\n",
    "                 dropout_value=0.,\n",
    "                 total_hidden_states=13\n",
    "                 ):\n",
    "        super(EmotionClassifierWeightedPooling, self).__init__()\n",
    "\n",
    "        self.start_layer = start_layer\n",
    "        self.classification = nn.Sequential(\n",
    "        )\n",
    "        for i in range(len(hidden_layers)):\n",
    "            curr = hidden_layers[i]\n",
    "            if i == 0:\n",
    "                self.classification.append(nn.LazyLinear(curr))\n",
    "                self.classification.append(activation_fn())\n",
    "                if dropout_value > 0:\n",
    "                    self.classification.append(nn.Dropout(dropout_value))\n",
    "                continue\n",
    "            prev = hidden_layers[i - 1]\n",
    "            self.classification.append(nn.Linear(prev, curr))\n",
    "            self.classification.append(activation_fn())\n",
    "            if dropout_value > 0:\n",
    "                self.classification.append(nn.Dropout(dropout_value))\n",
    "\n",
    "        self.classification.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        # self.classification.append(nn.Softmax(dim=-1))\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # wieght # 64, 61, 768\n",
    "        self.layer_weight = nn.Parameter(\n",
    "            torch.full(size=(total_hidden_states - start_layer, 1, 1, 1), fill_value=1, dtype=torch.float32,\n",
    "                       requires_grad=True))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        with torch.no_grad():\n",
    "            weight_factor = self.layer_weight.expand(hidden_states.size())\n",
    "            weighted_average = (weight_factor * hidden_states).sum(dim=0) / self.layer_weight.sum()\n",
    "        return self.classification(weighted_average[:, 0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:15.392345Z",
     "start_time": "2024-01-12T17:36:15.335583Z"
    }
   },
   "id": "989c985a962522c4"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    if isinstance(y_true, torch.Tensor):\n",
    "        y_true = y_true.detach().cpu().numpy()\n",
    "        y_pred = y_pred.detach().cpu().numpy().argmax(-1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1_score_value = f1_score(y_true, y_pred, average='weighted')\n",
    "    return accuracy, f1_score_value\n",
    "\n",
    "\n",
    "def decode(input_, y_true, y_pred, confidence, top_5=True):\n",
    "    global tokenizer, category_to_emotion\n",
    "\n",
    "    input_ = input_[0].detach().cpu()\n",
    "    y_true = y_true[0].detach().cpu().item()\n",
    "    y_pred = y_pred[0].detach().cpu().numpy()\n",
    "    true_confidence = confidence[0]\n",
    "    pred_confidence = y_pred.max()\n",
    "    top_5_emotions = None\n",
    "    if top_5:\n",
    "        index = y_pred.argsort()[-5:]\n",
    "        top_5_emotions = [category_to_emotion[i] for i in index]\n",
    "    y_pred = y_pred.argmax()\n",
    "    input_ = tokenizer.decode(input_)\n",
    "    y_true = category_to_emotion[y_true]\n",
    "    y_pred = category_to_emotion[y_pred]\n",
    "    return input_, y_true, y_pred, true_confidence, pred_confidence, top_5_emotions\n",
    "\n",
    "\n",
    "def conv_fn(x):\n",
    "    global category_to_emotion\n",
    "    return category_to_emotion[x]\n",
    "\n",
    "\n",
    "def top(y_true, y_pred, top_n=5):\n",
    "    y_true = y_true.detach().cpu().numpy()\n",
    "    y_pred = y_pred.detach().cpu().numpy()\n",
    "    y_pred = np.argsort(y_pred, axis=-1)[:, -5:]\n",
    "    return np.mean(np.isin(y_true[:, np.newaxis], y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:15.661110Z",
     "start_time": "2024-01-12T17:36:15.633698Z"
    }
   },
   "id": "9959d949806bfc76"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_val = np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val: float):\n",
    "        save = False\n",
    "        if val < self.best_val:\n",
    "            self.counter = 0\n",
    "            self.best_val = val\n",
    "            save = True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience, save\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: EmotionClassifierConcat|EmotionClassifierWeightedPooling,\n",
    "                 agglomerate: Agglomerate,\n",
    "                 model_name: str,\n",
    "                 device,\n",
    "                 dataloaders: tuple[DataLoader, DataLoader, DataLoader | None],\n",
    "                 optimizer=torch.optim.AdamW,\n",
    "                 scheduler=None,\n",
    "                 lr: float = 1e-3,\n",
    "                 output_dir: str = './RESULTS',\n",
    "                 patience: int = 10,\n",
    "                 clipping_norm= 0.1,\n",
    "                 verbose=0,\n",
    "                 ):\n",
    "        self.model_name = model_name\n",
    "        self.agglomerate = agglomerate\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = scheduler\n",
    "        self.verbose = verbose\n",
    "        self.train_loader, self.test_loader, self.val_loader = dataloaders\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.train_f1_score = []\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        self.validation_f1_score = []\n",
    "        self.validation_is_in_top_5 = []\n",
    "\n",
    "        self.early_stopper = EarlyStopping(patience)\n",
    "        self.clipping_norm = clipping_norm\n",
    "\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "            \n",
    "        \n",
    "    def process_data(self, batch):\n",
    "        batch = self.agglomerate(batch)\n",
    "        return self.model.forward(batch)\n",
    "        \n",
    "\n",
    "    def print_architecture(self, batch_size=64):  #TODO: NOT WORKING\n",
    "        print(summary(self.model.classification, input_size=(3072,), batch_size=batch_size, device=self.device))\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.train_f1_score = []\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        self.validation_f1_score = []\n",
    "        self.validation_is_in_top_5 = []\n",
    "\n",
    "    def draw_training_metrics(self, save=True, format='pdf', figsize=(16, 8), step=1):\n",
    "        if len(self.train_loss) == 0:\n",
    "            return\n",
    "        fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "        fig.suptitle(f'Training metrics')\n",
    "        axs[0].plot(self.train_loss, label='Train')\n",
    "        axs[0].plot(self.validation_loss, label='Validation')\n",
    "        axs[0].set_xlabel('Epochs')\n",
    "        axs[0].set_ylabel('Loss')\n",
    "        axs[0].set_xticks(np.arange(0, len(self.train_loss), step))\n",
    "        axs[0].legend()\n",
    "\n",
    "        axs[1].plot(self.train_accuracy, label='Train')\n",
    "        axs[1].plot(self.validation_accuracy, label='Validation')\n",
    "        axs[1].set_xlabel('Epochs')\n",
    "        axs[1].set_ylabel('Accuracy')\n",
    "        axs[1].set_xticks(np.arange(0, len(self.train_accuracy), step))\n",
    "        axs[1].legend()\n",
    "\n",
    "        axs[2].plot(self.train_f1_score, label='Train')\n",
    "        axs[2].plot(self.validation_f1_score, label='Validation')\n",
    "        axs[2].set_xlabel('Epochs')\n",
    "        axs[2].set_ylabel('F1 Score')\n",
    "        axs[2].set_xticks(np.arange(0, len(self.train_f1_score), step))\n",
    "        axs[2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(f'{self.output_dir}/{self.model_name}_train_metrics.{format}', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def draw_top_5(self, save=True, format='pdf', figsize=(16, 8), step=1):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        plt.plot(self.validation_is_in_top_5)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.tight_layout()\n",
    "        if save:\n",
    "            plt.savefig(f'{self.output_dir}/{self.model_name}_top_5.{format}', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def train_model(self, epochs):\n",
    "        self.reset_params()\n",
    "        loader_length = len(self.train_loader)\n",
    "        for ep in range(1, epochs + 1):\n",
    "            print('#'*20, ep, '#'*20)\n",
    "            self.model.train()\n",
    "            loss_ep: float = 0.\n",
    "            accuracy_ep: float = 0.\n",
    "            f1_score_ep: float = 0.\n",
    "            for batch, labels, confidence, in tqdm(self.train_loader):\n",
    "                out = self.process_data(batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss_fn(out, labels.to(self.device))\n",
    "                loss.backward()\n",
    "                if self.clipping_norm > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), self.clipping_norm)\n",
    "                self.optimizer.step()\n",
    "                loss_ep += loss.item()\n",
    "                with torch.no_grad():\n",
    "                    a, f1 = compute_metrics(labels, out)\n",
    "                    accuracy_ep += a\n",
    "                    f1_score_ep += f1\n",
    "            loss_ep /= loader_length\n",
    "            accuracy_ep /= loader_length\n",
    "            f1_score_ep /= loader_length\n",
    "            self.train_loss.append(loss_ep)\n",
    "            self.train_accuracy.append(accuracy_ep)\n",
    "            self.train_f1_score.append(f1_score_ep)\n",
    "            print(f'TRAIN LOSS: {loss_ep:.5f} ; ACCURACY: {accuracy_ep:.5f} ; F1: {f1_score_ep:.5f}')\n",
    "            if self.val_loader is not None:\n",
    "                if self.validate():\n",
    "                    break\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_val = 0\n",
    "            accuracies = 0\n",
    "            f1_scores = 0\n",
    "            length_val_dataloader = len(self.val_loader)\n",
    "            top_5_ep = 0.\n",
    "            for idx, (batch, labels, confidence) in enumerate(self.val_loader):\n",
    "                out = self.process_data(batch)\n",
    "                loss = self.loss_fn(out, labels.to(self.device))\n",
    "                loss_val += loss.item()\n",
    "                out = nn.functional.softmax(out, dim=-1)\n",
    "                ac, f1 = compute_metrics(labels,out)\n",
    "                accuracies += ac\n",
    "                f1_scores += f1\n",
    "                top_5_ep += top(labels, out)\n",
    "                if self.verbose:\n",
    "                    if idx == length_val_dataloader - 1:\n",
    "                        input_, y_true, y_pred, c_true, c_pred, top_5_emotions = decode(batch['input_ids'], labels, out,\n",
    "                                                                                        confidence)\n",
    "\n",
    "            loss_val /= length_val_dataloader\n",
    "            accuracies /= length_val_dataloader\n",
    "            f1_scores /= length_val_dataloader\n",
    "            top_5_ep /= length_val_dataloader\n",
    "            self.validation_loss.append(loss_val)\n",
    "            self.validation_accuracy.append(accuracies)\n",
    "            self.validation_f1_score.append(f1_scores)\n",
    "            self.validation_is_in_top_5.append(top_5_ep)\n",
    "            print(\n",
    "                f'VAL   LOSS: {loss_val:.5f} ; ACCURACY: {accuracies:.5f} ; F1: {f1_scores:.5f} ; top 5: {top_5_ep:.5f}')\n",
    "            if self.verbose:\n",
    "                print('INPUT:', input_.replace('[PAD]', '').rstrip())\n",
    "                print('TRUE LABEL:', y_true, 'conf:', f'{c_true:.5f}')\n",
    "                print('PREDICTED_LABEL', y_pred, 'conf', f'{c_pred:.5f}')\n",
    "                if top_5_emotions is not None:\n",
    "                    print(f'TOP 5: {list(reversed(top_5_emotions))}')\n",
    "            break_, save_ = self.early_stopper(loss_val)\n",
    "            if save_:\n",
    "                torch.save(self.model.state_dict(), f'{self.output_dir}/{self.model_name}.pt')\n",
    "        return break_\n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        if not path.endswith('.pt'):\n",
    "            path += '.pt'\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:16.139387Z",
     "start_time": "2024-01-12T17:36:16.137104Z"
    }
   },
   "id": "1b91de6968184e3f"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marziolunghi/Documents/GitHub/ATML-NLP/.venv/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_SIZE = len(emotions_to_category)\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "hidden_layers = [128, 64]\n",
    "\n",
    "# model = EmotionClassifierConcat(output_size=OUTPUT_SIZE,\n",
    "#                                 device=DEVICE,\n",
    "#                                 hidden_layers=hidden_layers)\n",
    "\n",
    "model = EmotionClassifierWeightedPooling(\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    device=DEVICE,\n",
    "    hidden_layers=hidden_layers,\n",
    "    start_layer=4\n",
    ")\n",
    "agglomerate = WeightedPooling(device=DEVICE, start_layer=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:28.827386Z",
     "start_time": "2024-01-12T17:36:16.269952Z"
    }
   },
   "id": "5d7b04f027f18dcb"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "model_name = 'test1'\n",
    "output_size = len(emotions_to_category)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "dataloaders = train_dataloader, test_dataloader, val_dataloader\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    agglomerate=agglomerate,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    dataloaders=dataloaders,\n",
    "    verbose=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:28.846731Z",
     "start_time": "2024-01-12T17:36:28.830426Z"
    }
   },
   "id": "ea98c2f19b79934f"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "# trainer.print_architecture(64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:36:28.850335Z",
     "start_time": "2024-01-12T17:36:28.848249Z"
    }
   },
   "id": "3f4c8b6d9d9b0bca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### 1 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "481c4e95dab444ea9e66eb3c8a65eba5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 3.56792 ; ACCURACY: 0.07096 ; F1: 0.04081\n",
      "VAL   LOSS: 3.32074 ; ACCURACY: 0.12480 ; F1: 0.09269 ; top 5: 0.81094\n",
      "INPUT: [CLS] what on earth could you be expecting? [SEP]\n",
      "TRUE LABEL: surprised conf: 0.64400\n",
      "PREDICTED_LABEL questioning conf 0.11047\n",
      "TOP 5: ['questioning', 'excited', 'furious', 'angry', 'suggesting']\n",
      "#################### 2 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23ca4789d7d5489dad821a0ed2aefff1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 3.14740 ; ACCURACY: 0.16670 ; F1: 0.13647\n",
      "VAL   LOSS: 2.94564 ; ACCURACY: 0.20508 ; F1: 0.18534 ; top 5: 0.97754\n",
      "INPUT: [CLS] i messed up, harry. i messed it all up. [SEP]\n",
      "TRUE LABEL: guilty conf: 0.58299\n",
      "PREDICTED_LABEL ashamed conf 0.07220\n",
      "TOP 5: ['ashamed', 'neutral', 'surprised', 'sympathizing', 'sad']\n",
      "#################### 3 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "383aa35ff3e84fd9ab4c6419bb6b85ff"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 2.82137 ; ACCURACY: 0.22383 ; F1: 0.19915\n",
      "VAL   LOSS: 2.64613 ; ACCURACY: 0.27910 ; F1: 0.26012 ; top 5: 0.99707\n",
      "INPUT: [CLS] nothing anymore. i was fired. [SEP]\n",
      "TRUE LABEL: devastated conf: 0.56908\n",
      "PREDICTED_LABEL devastated conf 0.21734\n",
      "TOP 5: ['devastated', 'sad', 'ashamed', 'lonely', 'angry']\n",
      "#################### 4 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3d3fd5b876d48138ead282b2e56daaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 2.61243 ; ACCURACY: 0.26393 ; F1: 0.24276\n",
      "VAL   LOSS: 2.54340 ; ACCURACY: 0.29082 ; F1: 0.26969 ; top 5: 0.99648\n",
      "INPUT: [CLS] almost shot him. [SEP]\n",
      "TRUE LABEL: guilty conf: 0.90212\n",
      "PREDICTED_LABEL ashamed conf 0.22093\n",
      "TOP 5: ['ashamed', 'disgusted', 'annoyed', 'surprised', 'terrified']\n",
      "#################### 5 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "049b4ecc6f7e4d499dcb54aee8fe0d27"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 2.48357 ; ACCURACY: 0.29289 ; F1: 0.27647\n",
      "VAL   LOSS: 2.46854 ; ACCURACY: 0.31055 ; F1: 0.28650 ; top 5: 0.99023\n",
      "INPUT: [CLS] you don't actually know if he read all of it. maybe he's telling the truth. [SEP]\n",
      "TRUE LABEL: apprehensive conf: 0.43369\n",
      "PREDICTED_LABEL apprehensive conf 0.20185\n",
      "TOP 5: ['apprehensive', 'hopeful', 'neutral', 'confident', 'afraid']\n",
      "#################### 6 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2bb035ee80e4e9b8f09e028c6a9f6d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN LOSS: 2.38831 ; ACCURACY: 0.31369 ; F1: 0.29790\n",
      "VAL   LOSS: 2.36975 ; ACCURACY: 0.31875 ; F1: 0.29922 ; top 5: 0.99902\n",
      "INPUT: [CLS] it feels so good... the sun lower than dope. [SEP]\n",
      "TRUE LABEL: content conf: 0.43387\n",
      "PREDICTED_LABEL joyful conf 0.15846\n",
      "TOP 5: ['joyful', 'impressed', 'sentimental', 'nostalgic', 'proud']\n",
      "#################### 7 ####################\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/157 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "368d22da2b3c4cd8b7ca6a87a02a6714"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train_model(20)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:36:28.852748Z"
    }
   },
   "id": "c7bd64b409a74161"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_training_metrics(save=False, figsize=(18, 6))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d1fe554cc813590b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.draw_top_5(save=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "361de169f6c17dfd"
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def dialogue(model: EmotionClassifierWeightedPooling | EmotionClassifierConcat, agglomerate:Agglomerate, deterministic=True):\n",
    "    global category_to_emotion\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            user_text = input('ENTER SENTENCE:')\n",
    "            if user_text.lower() == 'exit' or len(user_text) == 0:\n",
    "                print('BYE!')\n",
    "                return\n",
    "            print(f'USER: {user_text}')\n",
    "            assert isinstance(user_text, str)\n",
    "            batch = tokenizer(user_text, padding=True, return_tensors='pt')\n",
    "            batch = agglomerate(batch)\n",
    "            prediction = model.forward(batch)\n",
    "       \n",
    "            prediction = nn.functional.softmax(prediction, dim=-1).cpu().numpy().flatten()\n",
    "            if deterministic:\n",
    "                predicated_label = prediction.argmax()\n",
    "                probability = np.max(prediction)\n",
    "                predicated_label = category_to_emotion[predicated_label]\n",
    "                top_5_prediction = prediction.argsort()[-5:]\n",
    "                top_5_probability = prediction[top_5_prediction][::-1]\n",
    "                top_5 = np.vectorize(conv_fn)(top_5_prediction)[::-1]\n",
    "                dictionary = {key: value for key, value in zip(top_5, top_5_probability)}\n",
    "                print(f'BOT: {predicated_label} with prob: {probability}')\n",
    "                print(f'TOP 5: {dictionary}')\n",
    "            time.sleep(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:32:52.062160Z",
     "start_time": "2024-01-12T17:32:52.026031Z"
    }
   },
   "id": "88872b7ef4322a76"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Sara is gay.\n",
      "BOT: sentimental with prob: 0.07547243684530258\n",
      "TOP 5: {'sentimental': 0.07547244, 'devastated': 0.07370874, 'impressed': 0.072756805, 'proud': 0.06595751, 'terrified': 0.058032878}\n",
      "USER: Marzio is gay.\n",
      "BOT: impressed with prob: 0.09058864414691925\n",
      "TOP 5: {'impressed': 0.090588644, 'proud': 0.0695479, 'sentimental': 0.06849631, 'disgusted': 0.06001436, 'devastated': 0.05877}\n",
      "USER: Volodymyr is gay.\n",
      "BOT: impressed with prob: 0.08631984144449234\n",
      "TOP 5: {'impressed': 0.08631984, 'disgusted': 0.07631919, 'terrified': 0.07576108, 'devastated': 0.07470923, 'sentimental': 0.07248718}\n",
      "USER: Filippo is gay.\n",
      "BOT: impressed with prob: 0.08358092606067657\n",
      "TOP 5: {'impressed': 0.083580926, 'devastated': 0.07516315, 'sentimental': 0.074063286, 'disgusted': 0.06886729, 'terrified': 0.06863827}\n",
      "USER: Alessandro is gay.\n",
      "BOT: impressed with prob: 0.09584490954875946\n",
      "TOP 5: {'impressed': 0.09584491, 'sentimental': 0.08723568, 'proud': 0.07655967, 'devastated': 0.064022504, 'nostalgic': 0.058523588}\n",
      "USER: Nicola is gay.\n",
      "BOT: sentimental with prob: 0.07695534080266953\n",
      "TOP 5: {'sentimental': 0.07695534, 'proud': 0.07269675, 'impressed': 0.07144413, 'devastated': 0.06698443, 'terrified': 0.05848861}\n",
      "USER: Mythene is gay.\n",
      "BOT: impressed with prob: 0.0802258849143982\n",
      "TOP 5: {'impressed': 0.080225885, 'devastated': 0.07871203, 'disgusted': 0.07314911, 'terrified': 0.07014219, 'sentimental': 0.06813518}\n",
      "BYE!\n"
     ]
    }
   ],
   "source": [
    "dialogue(model, agglomerate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T17:34:17.800912Z",
     "start_time": "2024-01-12T17:32:53.133139Z"
    }
   },
   "id": "7558e9f281a66cd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.645416Z"
    }
   },
   "id": "f2163ae98c4c0fb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "def test_model(model: EmotionClassifierWeightedPooling | EmotionClassifierConcat, agglomerate:Agglomerate, deterministic=True):\n",
    "    global category_to_emotion\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        real_labels = []\n",
    "        out_labels = []\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        top_5_ = []\n",
    "        for idx, (batch, labels, confidence) in enumerate(test_dataloader):\n",
    "            batch = agglomerate(batch)\n",
    "            out = model.forward(batch)\n",
    "            ac, f1 = compute_metrics(labels,out)\n",
    "            accuracies.append(ac)\n",
    "            f1_scores.append(f1)\n",
    "            real_labels.append(labels.cpu().numpy())\n",
    "            out = nn.functional.softmax(out, -1)\n",
    "            top_5_.append(top(labels, out))\n",
    "            out_labels.append(out.argmax(-1).cpu().numpy())\n",
    "        \n",
    "    return real_labels, out_labels, accuracies, f1_scores, top_5_\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.646163Z"
    }
   },
   "id": "ad9069651787128e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "real_labels, out_labels, accuracies, f1_scores, top_5_ = test_model(model, agglomerate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.646877Z"
    }
   },
   "id": "f9cd16fe2ce4ea1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'MEAN ACCURACIES : {np.mean(accuracies)}')\n",
    "print(f'MEAN F1 SCORE   : {np.mean(f1_scores)}')\n",
    "print(f'MEAN TOP 5 SCORE: {np.mean(top_5_)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.647964Z"
    }
   },
   "id": "12c1c352146e7bcc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.649324Z"
    }
   },
   "id": "b95ae789f1b0007c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "x = list(map(lambda x: category_to_emotion[x], np.concatenate(real_labels, axis=0)))\n",
    "y = list(map(lambda x: category_to_emotion[x], np.concatenate(out_labels, axis=0)))\n",
    "# cm = confusion_matrix(np.concatenate(real_labels, axis=0), np.concatenate(out_labels, axis=0))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(x, y, \n",
    "                                               ax=ax, \n",
    "                                               xticks_rotation='vertical',\n",
    "                                               cmap=plt.cm.Blues)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.650372Z"
    }
   },
   "id": "7a71e48b8023cae9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(x, y))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.651007Z"
    }
   },
   "id": "a9c3c6e44ffb0848"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.651759Z"
    }
   },
   "id": "537f5197fc77c782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-12T17:24:46.652700Z"
    }
   },
   "id": "18b4c38aeda25fce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
