{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pickle\n",
    "import whisper\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to emotion\n",
    "\n",
    "For this part of the project we used the dataset \"IEMOCAP: Interactive emotional dyadic motion capture database.\"\n",
    "The dataset contains conversations audio, transcription, video and motion-capture.\n",
    "We discarded the transcription, video and motion-capture features and we used only the audio.\n",
    "\n",
    "We performed a transcription of the audio using Whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_to_text = whisper.load_model(\"base.en\", in_memory=True)\n",
    "def parse_audio(path):\n",
    "    audio = whisper.load_audio(path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(speech_to_text.device)\n",
    "    options = whisper.DecodingOptions(fp16=False, language=\"en\")\n",
    "    result = whisper.decode(speech_to_text, mel, options)\n",
    "    text = result.text\n",
    "    return [text, mel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [\"Session1\",\"Session2\",\"Session3\",\"Session4\", \"Session5\"]\n",
    "missing_sessions = []\n",
    "for i in sessions:\n",
    "    if not os.path.exists(f\"./{i}.pkl\"):\n",
    "        missing_sessions.append(i)\n",
    "\n",
    "for i in missing_sessions:\n",
    "    dir = f\"./{i}/dialog/EmoEvaluation/\"\n",
    "    files = os.listdir(dir)\n",
    "    files_dictionary = {}\n",
    "    for i in os.listdir(dir):\n",
    "        if i.endswith(\".txt\"):\n",
    "            file = open(dir + i, \"r\")\n",
    "            for line in file:\n",
    "                if line.startswith(\"[\"):\n",
    "                    line = line.split()\n",
    "                    path = \"_\".join(line[3].split(\"_\")[:-1])\n",
    "                    if path in files_dictionary:\n",
    "                        files_dictionary[path][line[3]] = [line[4]]\n",
    "                    else:\n",
    "                        files_dictionary[path] = {line[3]: [line[4]]}\n",
    "            file.close()\n",
    "    audio_path = f\"./{i}/sentences/wav/\"\n",
    "    for key in tqdm(files_dictionary):\n",
    "        for file in files_dictionary[key]:\n",
    "            path = audio_path + key + \"/\" + file + \".wav\"\n",
    "            files_dictionary[key][file] = files_dictionary[key][file] + parse_audio(path)\n",
    "    print(f\"Saving {i}\")\n",
    "    pickle.dump(files_dictionary, open(f\"{i}.pkl\", \"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session1 = pickle.load(open(\"Session1.pkl\", \"rb\"))\n",
    "session2 = pickle.load(open(\"Session2.pkl\", \"rb\"))\n",
    "session3 = pickle.load(open(\"Session3.pkl\", \"rb\"))\n",
    "session4 = pickle.load(open(\"Session4.pkl\", \"rb\"))\n",
    "session5 = pickle.load(open(\"Session5.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "\n",
    "We are only interested in emotions that are clearly identifiable.\n",
    "We proceeded in removing emotions that are not common and we removed al the emotions flagged as \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All emotions\n",
      "--------------------------------------------------\n",
      "ang 1103\n",
      "xxx 2507\n",
      "fru 1849\n",
      "neu 1708\n",
      "sur 107\n",
      "sad 1084\n",
      "exc 1041\n",
      "hap 595\n",
      "fea 40\n",
      "dis 2\n",
      "oth 3\n",
      "--------------------------------------------------\n",
      "filtered emotions\n",
      "--------------------------------------------------\n",
      "ang 1103\n",
      "xxx 2659\n",
      "fru 1849\n",
      "neu 1708\n",
      "sad 1084\n",
      "exc 1041\n",
      "hap 595\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "emotions = {}\n",
    "for i in [session1, session2, session3, session4, session5]:\n",
    "    for j in i.keys():\n",
    "        for k in i[j].keys():\n",
    "            if i[j][k][0] not in emotions.keys():\n",
    "                emotions[i[j][k][0]] = 1\n",
    "            else:\n",
    "                emotions[i[j][k][0]] += 1\n",
    "print(\"All emotions\")\n",
    "print(\"-\" * 50)\n",
    "for i in emotions.keys():\n",
    "    print(i, emotions[i])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"filtered emotions\")\n",
    "print(\"-\" * 50)\n",
    "emotions_to_remove = []\n",
    "for i in emotions.keys():\n",
    "    if emotions[i] < 150:\n",
    "        emotions_to_remove.append(i)\n",
    "        emotions[\"xxx\"] += emotions[i]\n",
    "for i in emotions_to_remove:\n",
    "    del emotions[i]\n",
    "for i in emotions.keys():\n",
    "    print(i, emotions[i])\n",
    "print(\"-\" * 50)\n",
    "for i in [session1, session2, session3, session4, session5]:\n",
    "    for j in i.keys():\n",
    "        for k in i[j].keys():\n",
    "            if i[j][k][0] in emotions_to_remove:\n",
    "                i[j][k][0] = \"xxx\"\n",
    "emotions = {}\n",
    "for i in [session1, session2, session3, session4, session5]:\n",
    "    for j in i.keys():\n",
    "        for k in i[j].keys():\n",
    "            if i[j][k][0] not in emotions.keys():\n",
    "                emotions[i[j][k][0]] = 1\n",
    "            else:\n",
    "                emotions[i[j][k][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points 7380\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "for i in [session1, session2, session3, session4, session5]:\n",
    "    for j in i.keys():\n",
    "        for k in i[j].keys():\n",
    "            if i[j][k][0] != \"xxx\":\n",
    "                all_data.append(i[j][k])\n",
    "print(\"Total data points\", len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "As in the previous part we extracted features from the text using BERT, we are using the output of the last four layers of the model and we are combining them together as input to the part of the network responsible for the text.\n",
    "For the part of the model responsible in extracting information from the audio we pass the mel spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAudioDataloader(Dataset):\n",
    "    def __init__(self, text_model, tokenizer, data):\n",
    "        self.text_model = text_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.emotion_to_one_hot = {\n",
    "            \"ang\": torch.tensor([1, 0, 0, 0, 0, 0], dtype=torch.float32),\n",
    "            \"fru\": torch.tensor([0, 1, 0, 0, 0, 0], dtype=torch.float32),\n",
    "            \"neu\": torch.tensor([0, 0, 1, 0, 0, 0], dtype=torch.float32),\n",
    "            \"sad\": torch.tensor([0, 0, 0, 1, 0, 0], dtype=torch.float32),\n",
    "            \"exc\": torch.tensor([0, 0, 0, 0, 1, 0], dtype=torch.float32),\n",
    "            \"hap\": torch.tensor([0, 0, 0, 0, 0, 1], dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with torch.no_grad():\n",
    "            emotion, text, audio = self.data[idx]\n",
    "            encoded_input = self.tokenizer(text, return_tensors=\"pt\").to(\n",
    "                self.text_model.device\n",
    "            )\n",
    "            output_text_features = self.text_model(**encoded_input)\n",
    "            all_hidden_states = torch.stack(output_text_features[\"hidden_states\"])\n",
    "            concatenate_pooling = torch.cat(\n",
    "                (\n",
    "                    all_hidden_states[-1],\n",
    "                    all_hidden_states[-2],\n",
    "                    all_hidden_states[-3],\n",
    "                    all_hidden_states[-4],\n",
    "                ),\n",
    "                -1,\n",
    "            )\n",
    "            concatenate_pooling = concatenate_pooling[:, 0]\n",
    "            return (\n",
    "                concatenate_pooling.squeeze(0).float(),\n",
    "                audio.unsqueeze(0),\n",
    "                self.emotion_to_one_hot[emotion],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.3 when it was built against 1.14.2, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.update({\"output_hidden_states\": True})\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "text_to_embeddings = AutoModel.from_pretrained(\n",
    "    \"bert-base-uncased\", config=config, torch_dtype=torch.float16\n",
    ")\n",
    "text_to_embeddings = text_to_embeddings.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextAudioDataloader(text_to_embeddings, tokenizer, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    dataset, [0.7, 0.2, 0.1]\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a simple neural network that given the audio and the text performs a classification of the emotion.\n",
    "\n",
    "We are using a CNN to extract features from the mel spectrogram and we are applying and a MLP for the output produced by BERT.\n",
    "The output of the two networks are then concatenated and an MLP is used to perform classification.\n",
    "\n",
    "We are going to test different combinations of the network:\n",
    "- Only audio\n",
    "- Only text\n",
    "- Text and audio\n",
    "- Text and audio, with a 20% probability of masking the audio during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class AudioAndTextModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes, dropout=0.0, text=False, audio=False, probability_removing_audio=0.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.num_classes = num_classes\n",
    "        self.text = text\n",
    "        self.audio = audio\n",
    "        self.probability_removing_audio = probability_removing_audio\n",
    "\n",
    "        if audio:\n",
    "            self.audio_model = nn.Sequential(\n",
    "                nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2)),\n",
    "                nn.SiLU(),\n",
    "                nn.MaxPool2d(kernel_size=(4, 4)),\n",
    "                nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),\n",
    "                nn.SiLU(),\n",
    "                nn.MaxPool2d(kernel_size=(4, 4)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1472, 512),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.SiLU(),\n",
    "            )\n",
    "\n",
    "        if text:\n",
    "            self.text_linear = nn.Sequential(\n",
    "                nn.Linear(3072, 256),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.SiLU(),\n",
    "            )\n",
    "\n",
    "        if text and audio:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(512, 256),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(256, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(256, num_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        if self.text and not self.audio:\n",
    "            features = self.text_linear(text)\n",
    "        elif self.audio and not self.text:\n",
    "            features = self.audio_model(audio)\n",
    "        else:\n",
    "            text_features = self.text_linear(text)\n",
    "            if random.random() < self.probability_removing_audio and self.training:\n",
    "                audio_features = torch.zeros_like(text_features)\n",
    "            else:\n",
    "                audio_features = self.audio_model(audio)\n",
    "            features = torch.cat((audio_features, text_features), dim=1)\n",
    "\n",
    "        return self.linear(features)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, audio, labels = batch\n",
    "        logits = self(audio, text)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, label_smoothing=0.3)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        accuracy = self.accuracy(logits, labels)\n",
    "        self.log(\n",
    "            \"train_acc\",\n",
    "            accuracy,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, audio, labels = batch\n",
    "        logits = self(audio, text)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        accuracy = self.accuracy(logits, labels)\n",
    "        self.log(\n",
    "            \"val_acc\", accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        top_2_accuracy = self.top_k_accuracy(logits, labels, k=2)\n",
    "        self.log(\n",
    "            \"val_top_2_acc\", top_2_accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        text, audio, labels = batch\n",
    "        logits = self(audio, text)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        self.log(\n",
    "            \"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        accuracy = self.accuracy(logits, labels)\n",
    "        self.log(\n",
    "            \"test_acc\", accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        top_2_accuracy = self.top_k_accuracy(logits, labels, k=2)\n",
    "        self.log(\n",
    "            \"test_top_2_acc\", top_2_accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        text, audio, labels = batch\n",
    "        logits = self(audio, text)\n",
    "        return torch.softmax(logits, dim=1)\n",
    "\n",
    "    def accuracy(self, logits, labels):\n",
    "        return torch.sum(\n",
    "            torch.argmax(logits, dim=1) == torch.argmax(labels, dim=1)\n",
    "        ).item() / len(labels)\n",
    "    \n",
    "    def top_k_accuracy(self, logits, labels, k=2):\n",
    "        y_true = torch.argmax(labels, dim=1).detach().cpu().numpy()\n",
    "        # print(y_true[:, np.newaxis])\n",
    "        y_pred = logits.detach().cpu().numpy()\n",
    "        y_pred = np.argsort(y_pred, axis=-1)[:, -k:]\n",
    "        # print(y_pred)\n",
    "        return np.mean(np.isin(y_true[:, np.newaxis], y_pred))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=5e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | text_linear | Sequential | 786 K \n",
      "1 | linear      | Sequential | 1.5 K \n",
      "-------------------------------------------\n",
      "788 K     Trainable params\n",
      "0         Non-trainable params\n",
      "788 K     Total params\n",
      "3.153     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3279e0ab68ee4971983e7c0033e86f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860e0999f8b348d6a3da4f00c2a40323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647a82ad9c2543d3a26bf2146dd1b4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd22a16b45ff49889d8a2a62bc74409c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae666f1369ed48af8ccc60412e602d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9098bfe4ba74a68ac5b6f3c74cd1b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02deee00ff14c0e8550427ff69bf32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed44c9a53e6d4d8090a26e3bd79a1387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e7b0c81a0d4f58bcea2a471680ee39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a878220916496fbf7b31fe66a8b4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ed8ed231ee42adb20918d93fa2ee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5dd163eff84811b0e1a81ac4218f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7946d6b9c0624468b18375e6e7f15787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ff5d24229344f5aa718ba8ec9c0eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5af82c12d04e029ce64aabbd3f6291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4ae3abc2a444f4b847d84d430185b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279f77b657414981a427a767f1afafc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dc103a43b54004ab7e695e7148ab49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447614bc2a9444fca5d2aee6ad3a45a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde350328bd94f3c8a7b79004bfceb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d358ef5c146497d894f6134962499e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863d02eabde642b7ac308d94d7ebe9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3924df6806341c78c3190ab73209f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75566b323e8b4063b1652a293a6594b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = AudioAndTextModel(6, text=True)\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"{epoch}-{val_acc:.2f}-{val_loss:.2f}.ckpt\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"mps\",\n",
    "    max_epochs=1000,\n",
    "    accumulate_grad_batches=4,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints/epoch=11-val_acc=0.51-val_loss=1.30.ckpt.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model weights from the checkpoint at /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints/epoch=11-val_acc=0.51-val_loss=1.30.ckpt.ckpt\n",
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40f5ee731304976bd189cd7ced46028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5094850659370422     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.3008297681808472     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_top_2_acc_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9932249188423157     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5094850659370422    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.3008297681808472    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_top_2_acc_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9932249188423157    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 1.3008297681808472,\n",
       "  'test_acc_epoch': 0.5094850659370422,\n",
       "  'test_top_2_acc_epoch': 0.9932249188423157}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | audio_model | Sequential | 890 K \n",
      "1 | linear      | Sequential | 1.5 K \n",
      "-------------------------------------------\n",
      "891 K     Trainable params\n",
      "0         Non-trainable params\n",
      "891 K     Total params\n",
      "3.567     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4b03628128470db89795c0651e532c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387e669158624a83a343739037fa957d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0158302ebb264b11b6684001baf768a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e9692443184736ab0eb43b43122a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7a6893a1fd4586bb1285f03bed5da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9caa71d27c24c76b5a9a1ade7de723b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8421be418fc64d5b8aea8c604db01502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a91048684e4244885f088ed92d27cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9413fe3e103e4977891712e093297562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225589ac96a84655bbcaa6cab56231e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ef905ea6b643b2a6db19e1bdd918a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e95752aa614c009c07c678da8ab204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a6dfd7dbd648e39750a8303ac8c9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fb67d59e97425d9dea6608874981ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5530f37e74bd4cc9a629444500afa4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773c36a2f59544b7893b2fd5b6313008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b84098aa6b4bb7bb3103d7565666c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a8052edd6540ffae8aaf6407dc1389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7321f645c7174f93a64473bcd39653a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b216b9211a4449adedcd31655d70d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "779c18684aae43a3b799d818835562ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c920c2ab1c94b73b12bda4141eb5ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8351afcd3bb94c72bb1d7e39a836a56b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14678058e1284b21aa6b9e9542858c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efe3ca8c4324c2cb013123f2a63a940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d133985eb34d13bc576c35ff26269e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc2ba52fced43a2a33b4e6ee6643dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints_audio/epoch=14-val_acc=0.41-val_loss=1.48.ckpt.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints_audio/epoch=14-val_acc=0.41-val_loss=1.48.ckpt.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca4d614af14485bbbe07c8219bcc5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_acc_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4146341383457184     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4836596250534058     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test_top_2_acc_epoch    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9471544623374939     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_acc_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4146341383457184    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4836596250534058    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test_top_2_acc_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9471544623374939    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 1.4836596250534058,\n",
       "  'test_acc_epoch': 0.4146341383457184,\n",
       "  'test_top_2_acc_epoch': 0.9471544623374939}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = AudioAndTextModel(6, audio=True)\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints_audio\",\n",
    "    filename=\"{epoch}-{val_acc:.2f}-{val_loss:.2f}.ckpt\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"mps\",\n",
    "    max_epochs=1000,\n",
    "    accumulate_grad_batches=4,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | audio_model | Sequential | 890 K \n",
      "1 | text_linear | Sequential | 786 K \n",
      "2 | linear      | Sequential | 132 K \n",
      "-------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.239     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc896cd885f246409087ae0ed91f77bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773dfe51b37548ca89084992b85bc50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ab688feb294a0c8122552738de57a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce90e78ae73244f5a88d8601c141acab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8de0eff501477fa6e4c6b53331da56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2463efd4de0c4c53900e6ed1cb3ea290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d86a8212a24d3b8a6fc0236e87054e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421d713049d74c46930ade296433aa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a2c9dfe83849d19db37ce96f06fab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce4f53aee0e406a8cc8f3eca1fba3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a591b8ea27fb4af6bf7468a5d32923d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "Restoring states from the checkpoint path at /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints_audio_text/epoch=6-val_acc=0.57-val_loss=1.19.ckpt.ckpt\n",
      "Loaded model weights from the checkpoint at /Users/vladimirkarpenko/Documents/git/ATML-NLP/checkpoints_audio_text/epoch=6-val_acc=0.57-val_loss=1.19.ckpt.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e28f18463ce490ebf0e28279630b561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = AudioAndTextModel(6, audio=True, text=True)\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints_audio_text\",\n",
    "    filename=\"{epoch}-{val_acc:.2f}-{val_loss:.2f}.ckpt\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"mps\",\n",
    "    max_epochs=1000,\n",
    "    accumulate_grad_batches=4,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = AudioAndTextModel(6, audio=True, text=True, probability_removing_audio=0.2)\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=10)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints_audio_text_prob_0.2\",\n",
    "    filename=\"{epoch}-{val_acc:.2f}-{val_loss:.2f}.ckpt\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_acc\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"mps\",\n",
    "    max_epochs=1000,\n",
    "    accumulate_grad_batches=4,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
